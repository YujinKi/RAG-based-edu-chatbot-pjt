"""
RAG Service - ê³ ìˆ˜ì¤€ RAG ê¸°ëŠ¥ ì œê³µ
EmbeddingServiceë¥¼ í™œìš©í•˜ì—¬ ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ êµ¬í˜„
"""

from typing import List, Dict, Optional, Any
import google.generativeai as genai
from google.generativeai.types import File
from fastapi import HTTPException
import re

from services.embedding_service import get_embedding_service
from services.pdf_service import get_pdf_loader
from services.search_service import get_search_service
from services.vector_db_service import get_vector_db_service
from services.quiz_service import get_quiz_service


class RAGService:
    """
    ê³ ìˆ˜ì¤€ RAG(Retrieval-Augmented Generation) ì„œë¹„ìŠ¤
    ë¬¸ì„œ ì—…ë¡œë“œ, ê²€ìƒ‰, ë‹µë³€ ìƒì„±ì„ í†µí•© ê´€ë¦¬
    """

    def __init__(self):
        """RAGService ì´ˆê¸°í™”"""
        self.embedding_service = get_embedding_service()
        self.pdf_loader = get_pdf_loader()
        self.search_service = get_search_service()
        self.vector_db_service = get_vector_db_service()
        self.conversation_history: Dict[str, List[Dict[str, str]]] = {}

    def create_knowledge_base(
        self,
        name: str,
        display_name: Optional[str] = None
    ) -> Dict[str, str]:
        """
        [DEPRECATED - NOT FUNCTIONAL]
        ì§€ì‹ ë² ì´ìŠ¤(Knowledge Base) ìƒì„± - Corpus APIëŠ” Python SDKì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŒ

        ëŒ€ì‹  vector_db_serviceë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

        Args:
            name: ì§€ì‹ ë² ì´ìŠ¤ ì´ë¦„
            display_name: í‘œì‹œ ì´ë¦„

        Returns:
            ìƒì„±ëœ ì§€ì‹ ë² ì´ìŠ¤ ì •ë³´
        """
        raise HTTPException(
            status_code=501,
            detail="Corpus API is not available in Google Generative AI Python SDK. Use vector_db_service instead."
        )

    def add_document_to_knowledge_base(
        self,
        file_path: str,
        knowledge_base_name: str,
        display_name: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        [DEPRECATED - NOT FUNCTIONAL]
        ì§€ì‹ ë² ì´ìŠ¤ì— ë¬¸ì„œ ì¶”ê°€ - Corpus APIëŠ” Python SDKì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŒ

        ëŒ€ì‹  vector_db_service.create_vector_store_from_text()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

        Args:
            file_path: ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ
            knowledge_base_name: ì§€ì‹ ë² ì´ìŠ¤ ì´ë¦„
            display_name: ë¬¸ì„œ í‘œì‹œ ì´ë¦„

        Returns:
            ì¶”ê°€ëœ ë¬¸ì„œ ì •ë³´
        """
        raise HTTPException(
            status_code=501,
            detail="Corpus API is not available in Google Generative AI Python SDK. Use vector_db_service instead."
        )

    def search_knowledge_base(
        self,
        query: str,
        knowledge_base_name: str,
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        [DEPRECATED - NOT FUNCTIONAL]
        ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰ - Corpus APIëŠ” Python SDKì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŒ

        ëŒ€ì‹  vector_db_service.search()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            knowledge_base_name: ê²€ìƒ‰í•  ì§€ì‹ ë² ì´ìŠ¤ ì´ë¦„
            top_k: ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        raise HTTPException(
            status_code=501,
            detail="Corpus API is not available in Google Generative AI Python SDK. Use vector_db_service instead."
        )

    def ask_question(
        self,
        question: str,
        file_uris: List[str],
        conversation_id: Optional[str] = None,
        model_name: str = "gemini-2.5-flash"
    ) -> Dict[str, Any]:
        """
        ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ì— ë‹µë³€ (ê°„ë‹¨í•œ RAG)

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            file_uris: ì°¸ì¡°í•  íŒŒì¼ URI ë¦¬ìŠ¤íŠ¸ (files/xxx í˜•ì‹)
            conversation_id: ëŒ€í™” ID (ëŒ€í™” ì´ë ¥ ê´€ë¦¬ìš©)
            model_name: ì‚¬ìš©í•  Gemini ëª¨ë¸

        Returns:
            ë‹µë³€ ì •ë³´
        """
        try:
            # íŒŒì¼ ê°ì²´ ê°€ì ¸ì˜¤ê¸°
            files = []
            for uri in file_uris:
                file_obj = self.embedding_service.get_file_by_name(uri)
                if file_obj:
                    files.append(file_obj)
                else:
                    print(f"âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {uri}")

            if not files:
                raise ValueError("ìœ íš¨í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

            # ëŒ€í™” ì´ë ¥ ê°€ì ¸ì˜¤ê¸°
            history = []
            if conversation_id and conversation_id in self.conversation_history:
                history = self.conversation_history[conversation_id]

            # ë‹µë³€ ìƒì„±
            result = self.embedding_service.generate_answer_simple(
                query=question,
                files=files,
                model_name=model_name
            )

            # ëŒ€í™” ì´ë ¥ ì €ì¥
            if conversation_id:
                if conversation_id not in self.conversation_history:
                    self.conversation_history[conversation_id] = []

                self.conversation_history[conversation_id].append({
                    "role": "user",
                    "content": question
                })
                self.conversation_history[conversation_id].append({
                    "role": "assistant",
                    "content": result["answer"]
                })

            result["conversation_id"] = conversation_id

            return result

        except Exception as e:
            print(f"âŒ ì§ˆë¬¸ ë‹µë³€ ì‹¤íŒ¨: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to answer question: {str(e)}"
            )

    def chat_with_documents(
        self,
        message: str,
        file_uris: List[str],
        conversation_id: str,
        model_name: str = "gemini-2.5-flash"
    ) -> Dict[str, Any]:
        """
        ë¬¸ì„œ ê¸°ë°˜ ì±„íŒ… (ëŒ€í™” ì´ë ¥ ìœ ì§€) + ì›¹ ê²€ìƒ‰ í†µí•©

        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            file_uris: ì°¸ì¡°í•  íŒŒì¼ URI ë¦¬ìŠ¤íŠ¸
            conversation_id: ëŒ€í™” ID
            model_name: ì‚¬ìš©í•  Gemini ëª¨ë¸

        Returns:
            ë‹µë³€ ì •ë³´
        """
        try:
            # íŒŒì¼ ê°ì²´ ê°€ì ¸ì˜¤ê¸°
            files = []
            for uri in file_uris:
                file_obj = self.embedding_service.get_file_by_name(uri)
                if file_obj:
                    files.append(file_obj)

            if not files:
                raise ValueError("ìœ íš¨í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

            # ëŒ€í™” ì´ë ¥ ê°€ì ¸ì˜¤ê¸°
            if conversation_id not in self.conversation_history:
                self.conversation_history[conversation_id] = []

            history = self.conversation_history[conversation_id]

            # ëª¨ë¸ ìƒì„±
            model = genai.GenerativeModel(model_name)

            # 1ë‹¨ê³„: FAISS ë²¡í„° DBì—ì„œ ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
            print(f"ğŸ” FAISS ë²¡í„° DBì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ ì¤‘...")
            vector_context = ""
            search_results_metadata = []  # ì¶œì²˜ ë©”íƒ€ë°ì´í„° ì €ì¥
            try:
                # íŒŒì¼ ì´ë¦„ ì¶”ì¶œ (ì²« ë²ˆì§¸ íŒŒì¼ ê¸°ì¤€)
                file_name = files[0].display_name if files else None

                if file_name:
                    search_results = self.vector_db_service.search(
                        query=message,
                        file_name=file_name,
                        top_k=5
                    )

                    if search_results:
                        vector_context = "\n\n".join([
                            f"[ê´€ë ¨ ë¬¸ì„œ ì²­í¬ {idx+1}] (ìœ ì‚¬ë„: {result['score']:.4f}):\n{result['text']}"
                            for idx, result in enumerate(search_results)
                        ])

                        # ë©”íƒ€ë°ì´í„° ì €ì¥ (ì¶œì²˜ ì¶”ì ìš©)
                        search_results_metadata = [
                            {
                                "chunk_id": idx + 1,
                                "text_preview": result['text'][:200] + "..." if len(result['text']) > 200 else result['text'],
                                "similarity_score": round(result['score'], 4),
                                "metadata": result.get('metadata', {})
                            }
                            for idx, result in enumerate(search_results)
                        ]

                        print(f"âœ… FAISSì—ì„œ {len(search_results)}ê°œ ì²­í¬ ê²€ìƒ‰ ì™„ë£Œ")
                    else:
                        print(f"âš ï¸ FAISSì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
                else:
                    print(f"âš ï¸ íŒŒì¼ ì´ë¦„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            except Exception as e:
                print(f"âš ï¸ FAISS ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
                vector_context = ""

            # 2ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ + íŒŒì¼ë¡œ ë‹µë³€ ì‹œë„
            system_prompt_stage1 = """ë‹¹ì‹ ì€ ì—…ë¡œë“œëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**ì—­í• :**
- ì œê³µëœ ë¬¸ì„œ ì²­í¬ì™€ íŒŒì¼ì˜ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ì´í•´í•˜ê³  ë‹µë³€í•©ë‹ˆë‹¤
- ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ì¼ê´€ëœ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤
- **ì¤‘ìš”**: ë¬¸ì„œì— ì¶©ë¶„í•œ ì •ë³´ê°€ ì—†ì–´ì„œ ë‹µë³€ì´ ë¶ˆì™„ì „í•˜ê±°ë‚˜ ë¶€ì •í™•í•  ìˆ˜ ìˆë‹¤ê³  íŒë‹¨ë˜ë©´, ë‹µë³€ ëì— ë°˜ë“œì‹œ "[SEARCH_NEEDED: ê²€ìƒ‰ì–´]" í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•´ì£¼ì„¸ìš”
  - ì˜ˆ: "[SEARCH_NEEDED: íŒŒì´ì¬ ìµœì‹  ë²„ì „ ê¸°ëŠ¥]"
- ë¬¸ì„œì— ì¶©ë¶„í•œ ì •ë³´ê°€ ìˆìœ¼ë©´ ì •ìƒì ìœ¼ë¡œ ë‹µë³€í•˜ê³  [SEARCH_NEEDED]ë¥¼ í‘œì‹œí•˜ì§€ ë§ˆì„¸ìš”
- í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤
"""

            # ëŒ€í™” ì´ë ¥ í¬í•¨
            conversation_text = "\n".join([
                f"{'ì‚¬ìš©ì' if msg['role'] == 'user' else 'AI'}: {msg['content']}"
                for msg in history
            ])

            # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸
            vector_info = f"\n\n**ë¬¸ì„œì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ ë‚´ìš©:**\n{vector_context}" if vector_context else ""

            user_prompt_stage1 = f"""**ì´ì „ ëŒ€í™”:**
{conversation_text if conversation_text else '(ì²« ëŒ€í™”ì…ë‹ˆë‹¤)'}
{vector_info}

**í˜„ì¬ ì§ˆë¬¸:** {message}

ìœ„ ì •ë³´ì™€ ëŒ€í™” ë§¥ë½ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”. ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ [SEARCH_NEEDED: ê²€ìƒ‰ì–´]ë¥¼ í‘œì‹œí•˜ì„¸ìš”.
"""

            # ì²« ë²ˆì§¸ ë‹µë³€ ìƒì„±
            print(f"ğŸ“„ ë¬¸ì„œ ë° ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ë‹µë³€ ìƒì„± ì¤‘...")
            content_stage1 = files + [system_prompt_stage1, user_prompt_stage1]
            response_stage1 = model.generate_content(content_stage1)
            initial_answer = response_stage1.text

            # [SEARCH_NEEDED] ë§ˆì»¤ í™•ì¸
            search_pattern = r'\[SEARCH_NEEDED:\s*([^\]]+)\]'
            search_match = re.search(search_pattern, initial_answer)

            search_used = False
            final_answer = initial_answer
            search_results_text = ""

            if search_match:
                # ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°
                search_query = search_match.group(1).strip()
                print(f"ğŸ” ë¬¸ì„œì— ì •ë³´ ë¶€ì¡± ê°ì§€. ì›¹ ê²€ìƒ‰ ìˆ˜í–‰: '{search_query}'")

                # Tavily ê²€ìƒ‰ ìˆ˜í–‰
                search_results = self.search_service.search_and_format(
                    query=search_query,
                    max_results=3,
                    search_depth="advanced"
                )
                search_results_text = search_results

                # 2ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ì™€ í•¨ê»˜ ìµœì¢… ë‹µë³€ ìƒì„±
                system_prompt_stage2 = """ë‹¹ì‹ ì€ ì—…ë¡œë“œëœ ë¬¸ì„œì™€ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€í•˜ëŠ” ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**ì—­í• :**
- ì—…ë¡œë“œëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•©ë‹ˆë‹¤
- ë¬¸ì„œì— ë¶€ì¡±í•œ ì •ë³´ëŠ” ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¡œ ë³´ì¶©í•©ë‹ˆë‹¤
- ë¬¸ì„œì™€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì •í™•í•˜ê³  í’ë¶€í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤
- ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™œìš©í–ˆìŒì„ ë‹µë³€ ëì— ëª…ì‹œí•©ë‹ˆë‹¤: "[ì›¹ ê²€ìƒ‰ ê²°ê³¼ í™œìš©ë¨]"
- í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤
"""

                user_prompt_stage2 = f"""**ì´ì „ ëŒ€í™”:**
{conversation_text if conversation_text else '(ì²« ëŒ€í™”ì…ë‹ˆë‹¤)'}

**í˜„ì¬ ì§ˆë¬¸:** {message}

**ì›¹ ê²€ìƒ‰ ê²°ê³¼:**
{search_results_text}

ìœ„ ë¬¸ì„œë“¤ê³¼ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹µë³€ ëì— "[ì›¹ ê²€ìƒ‰ ê²°ê³¼ í™œìš©ë¨]"ì„ í‘œì‹œí•˜ì„¸ìš”.
"""

                print(f"ğŸ“ ê²€ìƒ‰ ê²°ê³¼ì™€ í•¨ê»˜ ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...")
                content_stage2 = files + [system_prompt_stage2, user_prompt_stage2]
                response_stage2 = model.generate_content(content_stage2)
                final_answer = response_stage2.text
                search_used = True

                print(f"âœ… ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€ ìƒì„± ì™„ë£Œ")
            else:
                print(f"âœ… ë¬¸ì„œë§Œìœ¼ë¡œ ì¶©ë¶„í•œ ë‹µë³€ ìƒì„± ì™„ë£Œ")

            # [SEARCH_NEEDED] ë§ˆì»¤ ì œê±° (ìµœì¢… ë‹µë³€ì—ì„œ)
            final_answer = re.sub(search_pattern, '', final_answer).strip()

            # ëŒ€í™” ì´ë ¥ ì—…ë°ì´íŠ¸
            self.conversation_history[conversation_id].append({
                "role": "user",
                "content": message
            })
            self.conversation_history[conversation_id].append({
                "role": "assistant",
                "content": final_answer
            })

            print(f"ğŸ’¬ ëŒ€í™” {conversation_id}: {len(self.conversation_history[conversation_id])}ê°œ ë©”ì‹œì§€")

            return {
                "conversation_id": conversation_id,
                "message": message,
                "answer": final_answer,
                "sources": {
                    "files": [f.display_name for f in files],
                    "retrieved_chunks": search_results_metadata if search_results_metadata else [],
                    "web_search": search_used
                },
                "model": model_name,
                "history_length": len(self.conversation_history[conversation_id]),
                "search_used": search_used
            }

        except Exception as e:
            print(f"âŒ ì±„íŒ… ì‹¤íŒ¨: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to chat with documents: {str(e)}"
            )

    def get_conversation_history(self, conversation_id: str) -> List[Dict[str, str]]:
        """
        ëŒ€í™” ì´ë ¥ ì¡°íšŒ

        Args:
            conversation_id: ëŒ€í™” ID

        Returns:
            ëŒ€í™” ì´ë ¥ ë¦¬ìŠ¤íŠ¸
        """
        return self.conversation_history.get(conversation_id, [])

    def clear_conversation_history(self, conversation_id: str):
        """
        ëŒ€í™” ì´ë ¥ ì‚­ì œ

        Args:
            conversation_id: ëŒ€í™” ID
        """
        if conversation_id in self.conversation_history:
            del self.conversation_history[conversation_id]
            print(f"ğŸ—‘ï¸ ëŒ€í™” ì´ë ¥ ì‚­ì œ: {conversation_id}")

    def generate_quiz_from_document(
        self,
        file_uri: str,
        num_questions: int = 5,
        difficulty: str = "medium",
        question_type: str = "multiple_choice",
        model_name: str = "gemini-2.5-flash"
    ) -> Dict[str, Any]:
        """
        ë¬¸ì„œ ë‚´ìš© ê¸°ë°˜ í€´ì¦ˆ ìƒì„± (QuizService ì‚¬ìš©)

        Args:
            file_uri: ë¬¸ì„œ íŒŒì¼ URI
            num_questions: ìƒì„±í•  ë¬¸ì œ ìˆ˜
            difficulty: ë‚œì´ë„ (easy, medium, hard)
            question_type: ë¬¸ì œ ìœ í˜• (multiple_choice, true_false, fill_blank)
            model_name: ì‚¬ìš©í•  Gemini ëª¨ë¸

        Returns:
            ìƒì„±ëœ í€´ì¦ˆ ì •ë³´
        """
        try:
            # íŒŒì¼ ê°ì²´ ê°€ì ¸ì˜¤ê¸°
            file_obj = self.embedding_service.get_file_by_name(file_uri)
            if not file_obj:
                raise ValueError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_uri}")

            # QuizService ì‚¬ìš©
            quiz_service = get_quiz_service()
            questions = quiz_service.generate_quiz(
                file_obj=file_obj,
                num_questions=num_questions,
                difficulty=difficulty,
                question_type=question_type,
                model_name=model_name
            )

            print(f"âœ… í€´ì¦ˆ ìƒì„± ì™„ë£Œ: {len(questions)}ê°œ ë¬¸ì œ")

            return {
                "success": True,
                "questions": questions,
                "total_questions": len(questions),
                "difficulty": difficulty,
                "source_file": file_obj.display_name
            }

        except Exception as e:
            print(f"âŒ í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to generate quiz: {str(e)}"
            )


# Global RAGService instance (ì‹±ê¸€í†¤ íŒ¨í„´)
_rag_service_instance: Optional[RAGService] = None


def get_rag_service() -> RAGService:
    """
    RAGService ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜

    Returns:
        RAGService ì¸ìŠ¤í„´ìŠ¤
    """
    global _rag_service_instance

    if _rag_service_instance is None:
        _rag_service_instance = RAGService()

    return _rag_service_instance
